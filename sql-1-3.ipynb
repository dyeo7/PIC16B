{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dependent-vacuum",
   "metadata": {},
   "source": [
    "Quarto -- a versatile tool from R world! I have used a lot of its precursor, R Markdown back in the days -- even if I did not use R daily as a graduate student. Quarto is pretty similar to it with more functionalities. I make quizzes and exams with Quarto on RStudio. It seems like RStudio (now renamed Posit) guys' goal now is expanding to Python world.\n",
    "\n",
    "Project -- you have access to previous topics in PIC16B! [link](https://docs.google.com/spreadsheets/d/1oEajTpOU6YYm5NdVAeX33IZOAwgBU0ZmvAkDGFTCY7U/edit?usp=sharing)\n",
    "\n",
    "So far: merging, melting, pivoting;\n",
    "after groupby-- aggregation, transform, apply\n",
    "\n",
    "\n",
    "# Working with Databases\n",
    "\n",
    "### Takeaways for Today\n",
    "\n",
    "1. Reading large data sets into memory can be surprisingly slow, and in some cases completely impractical. \n",
    "2. Databases offer a convenient way to perform selective *queries* on our data, allowing us to retrieve only the data that we want at any given time. \n",
    "3. The `sqlite3` package enables us to work with databases using Python commands, and the `pandas` package makes it easy to read the results of database queries as data frames. \n",
    "\n",
    "## Intro \n",
    "\n",
    "In class, we will mostly work with the data that could easily fit in memory. Data sets of under 20 megabytes or so can be loaded easily on most modern laptops, although subsequent analysis activities may be slow. \n",
    "\n",
    "Larger data sets can be noticeably slow to even load into memory. So far, we worked with NOAA-GHCN data. When loaded in memory, it took up ~1 GB. This is not too big to fit in memory, but it's large enough that it's noticeably slow to even load into pandas. To accelerate the process, let's download the files to the computer now. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0dc50e-d33a-485b-b86d-70bf4eb9aeb0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# create folder named \"datafiles\" if it does not exist\n",
    "if not os.path.exists(\"datafiles\"): \n",
    "    os.mkdir(\"datafiles\")\n",
    "\n",
    "# download the files\n",
    "import urllib.request\n",
    "intervals = [f\"{10 * i + 1}-{10 * (i+1)}\" for i in range(190, 202)]\n",
    "for interval in intervals:\n",
    "    url = f\"https://raw.githubusercontent.com/PIC16B-ucla/24F/main/datasets/noaa-ghcn/decades/{interval}.csv\"\n",
    "    urllib.request.urlretrieve(url, f\"datafiles/{interval}.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008fb008-c1e8-4c58-a1b1-0fac096da562",
   "metadata": {},
   "source": [
    "Import pandas, and measure time for `read_csv()` part only:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7ed2b6-486e-4c5a-8e70-9f416f5af183",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a086242-e02d-448b-aaef-f1b407d9fb3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "intervals = [f\"{10 * i + 1}-{10 * (i+1)}\" for i in range(190, 202)]# quiz! 1901-1910 to 2011-2020.\n",
    "dfs = []\n",
    "for interval in intervals:\n",
    "    filepath = f\"datafiles/{interval}.csv\"\n",
    "    df = pd.read_csv(filepath)\n",
    "    dfs.append(df)\n",
    "df = pd.concat(dfs, axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "permanent-voluntary",
   "metadata": {},
   "source": [
    "Longer than half a second just to load the data set! \n",
    "\n",
    "Note: Today, I'm not even *down*loading the data set -- this is just how long it takes to move the data from my computer's SSD into RAM. There's no `prepare_df()`, either.\n",
    "\n",
    "We can get a pretty good estimate of how much RAM is required to store the data using a handy method of Pandas data frames: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c235ce32-b30c-4033-99cd-b9397e4aa3f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44619f6d-99ff-4f00-80dd-ad68fb490904",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c1ec52-ff86-4fc6-81e8-3db33259029f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155f5f8c-4f5e-4a69-a567-4ea43adae5eb",
   "metadata": {},
   "source": [
    "`df.memory_usage()` shows memory usage of each column in bytes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "charitable-softball",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.memory_usage(index=True).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06d12d6-de7f-47cb-909a-8ac4537e52ea",
   "metadata": {},
   "source": [
    "**POLL**: Which of the following is closest to this number?\n",
    "\n",
    "- A. 10 MB\n",
    "- B. 100 MB\n",
    "- C. 1 GB\n",
    "- D. 10 GB\n",
    "\n",
    "Compare this number to the size of the long format table we used Wednesday."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "therapeutic-formation",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "The data contains 1.35M rows and 14 columns. Altogether, these consume roughly ____MB of RAM. This is an appreciable fraction of the RAM on most laptops. In some cases, we might need to work with even larger data sets that don't fit in RAM at all. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stylish-carol",
   "metadata": {},
   "source": [
    "## Introducing Databases\n",
    "\n",
    "When dealing with large data sets like these, it's relatively rare that we absolutely *have* to operate on the entire data set. In most cases, we can work with parts of the data at a time. In the context of the temperature data, \n",
    "\n",
    "1. We might want temperature measurements only between years 1990-2020. \n",
    "2. We might want temperature measurements only for a certain set of countries - maybe in Asia, say. \n",
    "3. We might only want temperature measurements only in the month of March. \n",
    "4. We might want a random 1% of all the data. \n",
    "\n",
    "**Databases** provide us with a structured way to move subsets of data from storage into memory. Python has a built-in module called `sqlite3` which we can use to create, manipulate, and query databases. There's also a very handy `pandas` interface, enabling us to efficiently create `pandas` data frames containing exactly the data that we want. \n",
    "\n",
    "- Database vs. memory (RAM): large dataset doesn't fit in RAM, large database on persistant storage doesn't disappear when you restart your computer.\n",
    "\n",
    "- Database vs. text-based data file: database softwares are made for easy access/manipulation of data (\"querying\" the data). for example, no need to load csv file with 10 million rows just to look up one customer's information.\n",
    "\n",
    "A common approach in genomics: compress data for each genotypes in a large data file, and store location of each genotype in that file in a separate database\n",
    "\n",
    "### Creating and Populating Databases\n",
    "\n",
    "After importing the `sqlite3` module, the first thing we should do is connect to a database. In case the specified database does not exist, instantiating the connection will also create an empty database with the specified name.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crude-opportunity",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fundamental-victim",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(\"temps.db\") # create a database in current directory called temps.db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interim-lithuania",
   "metadata": {},
   "source": [
    "A quick check in our file directory reveals that we now have a file called `temps.db`. Great! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "funded-accreditation",
   "metadata": {},
   "source": [
    "There are many ways to add data to a database. Since we are already familiar with Pandas data frames, we'll make use of some extremely convenient functionality which allows us to directly write data frames to a database. \n",
    "\n",
    "__But wait!__ Wasn't the whole point of this to avoid reading in an entire data frame? Indeed! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e224ba-383e-4473-a935-74afded398ee",
   "metadata": {},
   "source": [
    "Let's process each csv file one at a time, using the `prepare_df` function we used in the previous lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc69ecc-7f20-4a6a-9ef5-56af8d92e919",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prepare_df(df):\n",
    "    \"\"\"\n",
    "    prepares a piece of wide format dataframe into a long format data frame\n",
    "    \"\"\"\n",
    "    # melt to the long format table\n",
    "    df = df.melt(\n",
    "        id_vars = [\"ID\", \"Year\"],\n",
    "        value_vars = [f\"VALUE{i}\" for i in range(1, 13)],\n",
    "        var_name = \"Month\",\n",
    "        value_name = \"Temp\"\n",
    "    )\n",
    "\n",
    "    # cleaning month and temp\n",
    "    df[\"Month\"] = df[\"Month\"].str[5:].astype(int)\n",
    "    df[\"Temp\"]  = df[\"Temp\"] / 100\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regulation-virginia",
   "metadata": {},
   "source": [
    "OK! We are finally ready to populate a *table* in our database. \n",
    "\n",
    "A relational database consists of several *table*s. They are data frame-like objects, represented in SQLite (a type of RDBMS, relational database management system) rather than in Python. The `df.to_sql()` method writes to a specified table in the database (the `conn` object from earlier). We need to specify `if_exists` to ensure that we add each piece to the table, rather than overwriting them each time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "present-travel",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "intervals = [f\"{10 * i + 1}-{10 * (i+1)}\" for i in range(190, 202)]\n",
    "for i, interval in enumerate(intervals):\n",
    "    filepath = f\"datafiles/{interval}.csv\"\n",
    "    df = pd.read_csv(filepath)\n",
    "    df = prepare_df(df)\n",
    "    df.to_sql(\"temperatures\", conn, if_exists = \"replace\" if i == 0 else \"append\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38752ed7-470d-45b1-9a7f-cc6e4d0944f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i, df in enumerate(df_iter):\n",
    "    df = prepare_df(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informed-weekly",
   "metadata": {},
   "source": [
    "Let's similarly add a table for the metadata in our database. This is a pretty small data set so we don't need to worry about reading it in by chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extraordinary-physics",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "filename = \"https://raw.githubusercontent.com/PIC16B-ucla/24F/refs/heads/main/datasets/noaa-ghcn/station-metadata.csv\"\n",
    "stations = pd.read_csv(filename)\n",
    "stations.to_sql(\"stations\", conn, if_exists = \"replace\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "announced-partner",
   "metadata": {},
   "source": [
    "Now we have a database containing two tables. Let's just check that this is indeed the case. \n",
    "<!--\n",
    "SELECT name FROM sqlite_master WHERE type='table'\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worldwide-yugoslavia",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cursor = conn.cursor()\n",
    "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table'\")\n",
    "print(cursor.fetchall())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "available-ability",
   "metadata": {},
   "source": [
    "Ok wait -- there's actually a lot that just happened here! \n",
    "\n",
    "First: the *cursor* is our primary way to interact with the database. The cursor `execute`s *SQL commands*. Structured Query Language (SQL) is actually its own mini-programming language specifically designed for interacting with databases. We do need to learn a bit of SQL in order to work with databases.\n",
    "\n",
    "Here are some resources:\n",
    "\n",
    "- https://www.w3schools.com/sql/\n",
    "\n",
    "- https://www.sqltutorial.org/\n",
    "\n",
    "- https://www.w3schools.com/sql/sql_quickref.asp\n",
    "\n",
    "- https://www.sqltutorial.org/sql-cheat-sheet/\n",
    "\n",
    "or you can just get the hang of the basics and google the rest as you need, which is what 99% of people do.\n",
    "\n",
    "\n",
    "\n",
    "Now let's take a look at the actual command that we `execute`d.\n",
    "\n",
    "```sql\n",
    "SELECT name FROM sqlite_master WHERE type='table'\n",
    "```\n",
    "\n",
    "Let's break this down: \n",
    "\n",
    "- `SELECT name`: Show me the entries in the `name` column\n",
    "- `FROM sqlite_master`: of the `sqlite_master` table\n",
    "- `WHERE type='table'`: subject to the condition that the entry in the `type` column of `sqlite_master` is equal to ``table`` (i.e. don't include other kinds of objects)\n",
    "\n",
    "Finally, `cursor.fetchall()` returns the a list containing all the items returned by the query, which we then print.\n",
    "\n",
    "The special `sqlite_master` table contains information on all the objects in the database. We don't usually query it when we want to obtain data, but we can query it to learn about what kinds of tables we have, what their columns are, etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "numeric-framework",
   "metadata": {},
   "source": [
    "Let's get more detailed information about the items in each table. For example, we can inspect the column names and data types in each. This is a good way to check that we actually populated our database correctly. \n",
    "\n",
    "Let select the column `sql` column from each table names in the master table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "olive-thinking",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cursor.execute(\"SELECT sql FROM sqlite_master WHERE type='table';\")\n",
    "\n",
    "for result in cursor.fetchall():\n",
    "    print(result[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "governing-shelf",
   "metadata": {},
   "source": [
    "This looks pretty good! We have two tables, called `temperatures` and `stations`. The column names are what we would expect. Notice that `sql` has automatically inferred the data types, such as `REAL` and `TEXT`, from the input. Handy! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "combined-anger",
   "metadata": {},
   "source": [
    "#### Basic Queries\n",
    "\n",
    "Now we're ready to perform some basic queries on the data tables themselves. \n",
    "\n",
    "What data do we have for the year 1990? In `pandas` we would do this like: \n",
    "\n",
    "\n",
    "The SQL syntax is very different, but contains all the same ideas. \n",
    "\n",
    "- `SELECT`, like the syntax `temperatures[\"id\"]`, controls which column(s) will be returned. \n",
    "- `FROM` tells us which table to return columns from. \n",
    "- `WHERE` is like the Boolean index `[temperatures[\"year\"] == 1990]`. Only rows in which this criterion is satisfied will be returned. \n",
    "\n",
    "**NOTE**: SQL commands are case-insensitive, but it's considered good practice to place SQL keywords in ALL CAPS while names of columns and other content goes in lowercase. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feda5321-03f1-4b65-bd2b-32fb9e7abba5",
   "metadata": {},
   "source": [
    "Let's quickly go over some of the basic SQL queries using the following data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a953239-7da6-49d0-a674-d013aa78093e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "employees = pd.DataFrame(data={\"name\"   : [\"Alice\", \"Bob\", \"Carol\", \"Dave\", \"Eve\", \"Frank\"],\n",
    "                    \"email\"  : [\"alice@company.com\", \"bob@company.com\",\n",
    "                               \"carol@company.com\", \"dave@company.com\",\n",
    "                               \"eve@company.com\",   \"frank@comany.com\"],\n",
    "                    \"salary\" : [52000, 40000, 30000, 33000, 44000, 37000],\n",
    "                    \"dept\"   : [\"Accounting\", \"Accounting\", \"Sales\",\n",
    "                               \"Accounting\", \"Sales\", \"Sales\"]\n",
    "                              }\n",
    "                        )    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1e072f-e607-4fec-9659-d6780ef29a53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "employees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a79f01-6b38-402e-b0eb-9cc16a85b53c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "phone = pd.DataFrame(data={\n",
    "                            \"name\"  : [\"Bob\", \"Carol\", \"Eve\", \"Frank\"],\n",
    "                            \"phone\" : [\"919 555-1111\", \"919 555-2222\", \"919 555-3333\", \"919 555-4444\"]\n",
    "    }\n",
    ")\n",
    "phone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42ce079-3604-4843-a783-360340cbbf01",
   "metadata": {},
   "source": [
    "Adding the tables to the database `employees.sqlite`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35eb98a-017c-4179-bc21-b37a3184b792",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with sqlite3.connect(\"employees.sqlite\") as conn:\n",
    "    employees.to_sql(\"employees\", conn, if_exists = \"replace\", index = False)\n",
    "    phone.to_sql(\"phone\", conn, if_exists = \"replace\", index = False)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589462c8-9137-4ede-90dd-355f21b2ce66",
   "metadata": {},
   "source": [
    "We can get the list of tables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997d966a-0c6a-4ca4-abfe-b35c066c6649",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with sqlite3.connect(\"employees.sqlite\") as conn:\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table'\")\n",
    "    print(cursor.fetchall())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf7d7df-45c6-4899-9b3b-c59b1b7cfb0e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with sqlite3.connect(\"employees.sqlite\") as conn:\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"SELECT sql FROM sqlite_master WHERE type='table'\")\n",
    "    for c in cursor.fetchall():\n",
    "        print(c[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f480d21-1fea-4202-b7d3-b1ed7115e52e",
   "metadata": {},
   "source": [
    "`pd.read_sql_query()` is the function to take the result into a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7e0e41-8b1f-4d12-bd6c-e28643ca07b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with sqlite3.connect(\"employees.sqlite\") as conn:\n",
    "    df = pd.read_sql_query(\"SELECT * FROM employees\", conn)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9b4cc0-c1e2-47e6-bc60-2ca85943d75c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with sqlite3.connect(\"employees.sqlite\") as conn:\n",
    "    df = pd.read_sql_query(\"SELECT * FROM phone\", conn)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541f75c5-ff0b-4e7f-8b62-d5598e89b60e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with sqlite3.connect(\"employees.sqlite\") as conn:\n",
    "    df = pd.read_sql_query(\"SELECT name AS first_name, salary FROM employees;\", conn)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae164cae-1e1a-4661-8e12-8e47aa5bdb74",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with sqlite3.connect(\"employees.sqlite\") as conn:\n",
    "    df = pd.read_sql_query(\"SELECT name AS first_name, salary FROM employees ORDER BY salary;\", conn)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca09955-4e7c-462f-85d4-cc756993b7b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with sqlite3.connect(\"employees.sqlite\") as conn:\n",
    "    df = pd.read_sql_query(\"SELECT name AS first_name, salary FROM employees ORDER BY salary DESC;\", conn)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0803f5-ed9f-40b6-ac12-b44a1a9d8127",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with sqlite3.connect(\"employees.sqlite\") as conn:\n",
    "    df = pd.read_sql_query(\"SELECT * FROM employees WHERE salary < 40000;\", conn)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1915cfca-af16-4b30-80e5-75a233dd47ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with sqlite3.connect(\"employees.sqlite\") as conn:\n",
    "    df = pd.read_sql_query(\"SELECT * FROM employees GROUP BY dept;\", conn)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4751de6a-16da-40b2-a962-96fa069628fd",
   "metadata": {},
   "source": [
    "Aggregation by group. There are a number of aggregation functions predefined in SQL. See, for example: https://www.w3schools.com/sql/sql_aggregate_functions.asp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9cf50a-b9a6-49ff-9ef7-4b06e05bd076",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with sqlite3.connect(\"employees.sqlite\") as conn:\n",
    "    df = pd.read_sql_query(\"SELECT dept, AVG(salary) AS avg_salary FROM employees GROUP BY dept;\", conn)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c5b001-b18d-4733-b3f4-f3d555a0a453",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with sqlite3.connect(\"employees.sqlite\") as conn:\n",
    "    df = pd.read_sql_query(\"SELECT dept, COUNT(*) AS num_employees FROM employees GROUP BY dept;\", conn)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df119d94-6ae1-4623-bb26-88ba3a525edf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with sqlite3.connect(\"employees.sqlite\") as conn:\n",
    "    df = pd.read_sql_query(\"SELECT * FROM employees LIMIT 3;\", conn)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1fd256-210a-4c00-8dbb-b6b4892210c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with sqlite3.connect(\"employees.sqlite\") as conn:\n",
    "    df = pd.read_sql_query(\"SELECT * FROM employees ORDER BY name DESC LIMIT 3;\", conn)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c274d0d6-cf8e-407f-9b0d-f300cea9db45",
   "metadata": {},
   "source": [
    "**POLL** Then, what would be the SQL equivalent of the following command?\n",
    "\n",
    "```python\n",
    "temperatures[\"id\"][temperatures[\"year\"] == 1990]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c072e0ae-8e25-493b-8a39-6399d2947869",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "00f306c8-eaa3-4678-af1f-54dd54bf4010",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "stuffed-texas",
   "metadata": {},
   "source": [
    "This isn't very useful when we are just querying a single column, but we can also get multiple columns, in which case the tuple representation makes a bit more sense. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "egyptian-basin",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "relative-belief",
   "metadata": {},
   "source": [
    "We can include multiple criteria in `WHERE` using the Boolean operators `AND` and `OR`. How do we take the temperature data from January 1990?\n",
    "<!-- SELECT id, temp FROM temperatures WHERE year=1990 AND month=1; -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "taken-reason",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cursor.execute(\"SQL COMMAND HERE\")\n",
    "result = [cursor.fetchone() for i in range(10)]\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arctic-tuning",
   "metadata": {},
   "source": [
    "Which stations are either far south or far north? \n",
    "<!--SELECT id FROM stations WHERE latitude > 80 or latitude< -80 -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "raised-keyboard",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cursor.execute(\"SQL COMMAND HERE\")\n",
    "result = [cursor.fetchone() for i in range(10)]  # get just the first 10 results\n",
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:PIC16B-24W]",
   "language": "python",
   "name": "conda-env-PIC16B-24W-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
