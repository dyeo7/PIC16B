{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74f79602-1c06-4c70-af63-b13ad19a2d52",
   "metadata": {},
   "source": [
    "# DuckDB and Polars\n",
    "\n",
    "So far, we mostly talked about `pandas` and used databased through the SQLite software through the built-in `sqlite3` module.\n",
    "What are the limitations of them?\n",
    "\n",
    "Can we do better?\n",
    "\n",
    "For SQL-based data analytics, there are many open-source or commercially-available software, like MySQL, PostgreSQL, MongoDB, etc. However, there are a couple of recent, open-source, high-performance packages I wanted to discuss. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1730097f-ea38-4a21-b0d4-8231d1b861ca",
   "metadata": {},
   "source": [
    "## DuckDB\n",
    "\n",
    "Dr. Hua Zhou, my postdoctoral mentor and the director of the Master of Data Science in Health program, wrote in his BIOSTAT 203B (Intro to Data Science with R) lecture note: \n",
    "\n",
    "> BTW, as modern data scientists, we should all start using DuckDB (https://duckdb.org/) instead of SQLite. DuckDB is a modern, embeddable SQL OLAP (Online Analytical Processing) database management system. It is designed to handle analytical workloads (OLAP) on read-only data. It is based on a column-store architecture and is designed to be very fast to query, highly compressible, and run on modern hardware. It is a great alternative to SQLite for analytical workloads.\n",
    "\n",
    "* OLAP: another word for analytical query workloads, short for \"[online analytical processing](https://en.wikipedia.org/wiki/Online_analytical_processing)\". Pretty much the software supporting the operations we used in the classroom for analyzing data.\n",
    "\n",
    "This package does not require you to save your data in a database form. In Python, it can read directly from (compressed) CSV, parquet, and even pandas dataframe. Its performance is remarkable, using multiple threads for reading data file and running the queries.\n",
    "\n",
    "A quick example: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12b3a2c-3800-432e-a0f0-2b3f2c0ea1be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "duckdb.sql(\"INSTALL sqlite\") # we need sqlite plugin first\n",
    "with duckdb.connect(\"employees.sqlite\") as conn:\n",
    "    joined = conn.sql(\"SELECT * FROM employees LEFT JOIN phone ON employees.name = phone.name;\").df()\n",
    "joined"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eea2f67-0e1e-4178-80f9-0fb853a2a7cb",
   "metadata": {},
   "source": [
    "We can run a SQL command on a pandas dataframe!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8c88e7-3f1a-4b46-b792-01eed02f1098",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with sqlite3.connect(\"employees.sqlite\") as conn:\n",
    "    employees = pd.read_sql_query(\"SELECT * FROM employees;\", conn)\n",
    "    phone = pd.read_sql_query(\"SELECT * FROM phone;\", conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db67b6b-c777-49d9-92fc-10126d8333af",
   "metadata": {},
   "source": [
    "Note that `employees` and `phone` refer to pandas dataframes at this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5edc5bb0-a122-4cb9-9cf0-3d056e729215",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "duckdb.sql(\"SELECT * FROM employees LEFT JOIN phone ON employees.name = phone.name;\").df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40052ce-d2ec-4a06-a0a3-9aa8f0b7e36d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "with duckdb.connect(\"temps.db\") as conn:\n",
    "    # conn is automatically closed when this block ends\n",
    "    cmd = \\\n",
    "    \"\"\"\n",
    "    SELECT S.NAME, T.Month, ROUND(AVG(T.Temp), 1) \"Mean Temperature\"\n",
    "    FROM temperatures T\n",
    "    LEFT JOIN stations S ON T.ID = S.ID\n",
    "    WHERE S.LATITUDE>80 OR S.LATITUDE<-80\n",
    "    GROUP BY S.NAME, T.Month\n",
    "    ORDER BY S.NAME\n",
    "    \"\"\"\n",
    "    df = conn.sql(cmd).df()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86367834-5419-4c63-8405-2be27c147e81",
   "metadata": {},
   "source": [
    "Compare this number to the previous lecture note. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051c9a54-d19f-45b2-9c0c-07dd83a3bfd6",
   "metadata": {},
   "source": [
    "## Can we go even faster?\n",
    "\n",
    "It depends. If what you have is the full data, and is unlikely to be changed, it's worth trying saving them in a format that stores the data columnwise. That is how these software work with the data (think of pandas dataframes)! So if data are already stored in this format, DuckDB or polars work faster than working on data stored row-wise (such as CSV files or SQLite databases). Software discussed today both take advantage of columnar data storage, with vectorized operations. \n",
    "\n",
    "We install the parquet support for DuckDB first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53917192-b104-494b-a71e-8e7c74cfbda2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "duckdb.sql(\"INSTALL parquet;\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dbb8ebd-116e-469c-b5df-f9ed6e3d5a75",
   "metadata": {},
   "source": [
    "And this is the command to save the `temperatures` table into the `parquet` format. As you might expect, DuckDB can write data in many different formats. By the way, Pandas has support for the `parquet` format that works after installing pandas' optional dependency. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbda7f65-b2bf-4981-9e8e-e5986be5bf7d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with duckdb.connect(\"temps.db\") as conn:\n",
    "    conn.sql(\"\"\"\n",
    "    COPY\n",
    "        (SELECT * FROM temperatures)\n",
    "        TO 'temps.parquet'\n",
    "        (FORMAT 'parquet');\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec279c52-cf48-436f-ac47-ac25636e475e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with duckdb.connect(\"temps.db\") as conn:\n",
    "    stations = conn.sql(\"SELECT * from stations\").df()\n",
    "stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e7554c-18d5-46a1-9c99-5ce3a8742f76",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cmd = \\\n",
    "\"\"\"\n",
    "SELECT S.NAME, T.Month, ROUND(AVG(T.Temp), 1) \"Mean Temperature\"\n",
    "FROM 'temps.parquet' T\n",
    "LEFT JOIN stations S ON T.ID = S.ID\n",
    "WHERE S.LATITUDE>80 OR S.LATITUDE<-80\n",
    "GROUP BY S.NAME, T.Month\n",
    "ORDER BY S.NAME\n",
    "\"\"\"\n",
    "df = duckdb.sql(cmd).df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26021b6c-980e-487e-9cc3-a328e9d2b3e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%timeit df = duckdb.sql(cmd).df()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81a24b5-383e-440b-9662-54e73f18b1db",
   "metadata": {},
   "source": [
    "## Polars\n",
    "\n",
    "Polars is another tool you will need to keep your eyes on. While DuckDB is optimized for SQL-type queries, polars is optimized for `pandas`-style Python commands, thus being more flexible. People with stronger programming background might prefer this one over DuckDB. The backend of polars is written in the modern Rust language, suitable for effective parallel computing. Let's install the newest version of `polars`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341fd07a-1745-492c-a7da-f5f6664a37d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install polars --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7812eee7-995a-4e6d-8cc2-ef7297336d00",
   "metadata": {},
   "source": [
    "While this type of installation in general is often less recommended in this course, it's polars guys' preferred way of installation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c909d779-1185-43cd-b1de-f6ef2a0e57af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import polars as pl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17cbdc24-6681-4c87-982a-ae94283e3df6",
   "metadata": {},
   "source": [
    "Let's look back at how we processed our data from the beginning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d3aed9-a5f1-4249-bffa-1d5d32960e89",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "intervals = [f\"{10 * i + 1}-{10 * (i+1)}\" for i in range(190, 202)]# quiz! 1901-1910 to 2011-2020.\n",
    "dfs = []\n",
    "for interval in intervals:\n",
    "    filepath = f\"datafiles/{interval}.csv\"\n",
    "    df = pd.read_csv(filepath)\n",
    "    dfs.append(df)\n",
    "df = (pd.concat(dfs, axis=0, ignore_index=True)\n",
    "      .melt(\n",
    "        id_vars = [\"ID\", \"Year\"],\n",
    "        value_vars = [f\"VALUE{i}\" for i in range(1, 13)],\n",
    "        var_name = \"Month\",\n",
    "        value_name = \"Temp\")\n",
    "      .query(\"~Temp.isnull()\") \n",
    "      .assign(Month = lambda x : x.Month.str[5:].astype(int))\n",
    "      .assign(Temp = lambda x : x.Temp / 100)\n",
    "      .merge(stations, on=\"ID\", how=\"inner\")\n",
    "      .query(\"LATITUDE > 80 | LATITUDE < -80\")\n",
    "      .groupby([\"NAME\", \"Month\"])\n",
    "      .agg({\"Temp\": \"mean\"})\n",
    "      .rename(columns={\"Temp\": \"Mean Temp\"})\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65621958-5454-4f6f-9cb3-6e075193125a",
   "metadata": {},
   "source": [
    "I intentionally put some unfamiliiar methods for you here, but you can figure it out that the code block above does what we did in the past week. Whew, it's just a single long chain of pandas methods after reading the file! If you have seen R's `dplyr` syntaxes, and how they do `pipe`s, it's pretty similar. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b937c4d-c821-4553-9304-ae60ee2b64ac",
   "metadata": {},
   "source": [
    "Polars uses similar syntaxes, but it's much faster! It uses all the cores on your computer, and several datatypes are optimized through specialized internal representations known as the [Arrow](https://arrow.apache.org/docs/format/Columnar.html) format. The code above in polars is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f07fbf-f8de-4421-b543-afd28f54b3c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "df = pl.read_csv(\"datafiles/*.csv\")\n",
    "stations_pl = pl.from_pandas(stations) # changing pandas dataframe into polars one.\n",
    "df = (pl.read_csv(\"datafiles/*.csv\")\n",
    "        .unpivot(index=[\"ID\", \"Year\"], # melt\n",
    "              variable_name=\"Month\",\n",
    "              value_name=\"Temp\")\n",
    "        .filter(~pl.col(\"Temp\").is_null()) # removing null values\n",
    "        .with_columns(pl.col(\"Month\").str.slice(5).cast(pl.Int64))\n",
    "        .with_columns((pl.col(\"Temp\") / 100)) # divide the Temp column by 100\n",
    "        .join(stations_pl, on=\"ID\", how=\"inner\")\n",
    "        .filter((pl.col(\"LATITUDE\") > 80) | (pl.col(\"LATITUDE\")< -80))\n",
    "        .group_by([\"NAME\", \"Month\"])\n",
    "        .agg(pl.col(\"Temp\").mean().alias(\"Mean Temp\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db06463-0aea-441a-a80e-417eab4441c2",
   "metadata": {},
   "source": [
    "Polars is a DataFrame interface on top of an OLAP Query Engine implemented in Rust"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69539fec-4d5d-40d2-980f-3b0f43d41178",
   "metadata": {},
   "source": [
    "Polars, as well as DuckDB, supports \"lazy evaluation\": the command to run can is synthesized before running the actual computation, and it can run later. This provides an opportunity for the software to optimize the computation before it is run. This is done through the type `LazyFrame`: accessible by using functions starting with `scan_*` rather than `read_*`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8fa4766-585a-4688-8749-963e427c7eae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stations_pl = pl.scan_csv(\"station-metadata.csv\")\n",
    "df = (pl.scan_csv(\"datafiles/*.csv\")\n",
    "        .unpivot(index=[\"ID\", \"Year\"], # melt\n",
    "              variable_name=\"Month\",\n",
    "              value_name=\"Temp\")\n",
    "        .filter(~pl.col(\"Temp\").is_null()) # removing null values\n",
    "        .with_columns(pl.col(\"Month\").str.slice(5).cast(pl.Int64))\n",
    "        .with_columns((pl.col(\"Temp\") / 100)) # divide the Temp column by 100\n",
    "\n",
    "        .join(stations_pl, on=\"ID\", how=\"inner\")\n",
    "        .filter((pl.col(\"LATITUDE\") > 80) | (pl.col(\"LATITUDE\")< -80))\n",
    "        .group_by([\"NAME\", \"Month\"])\n",
    "        .agg(pl.col(\"Temp\").mean().alias(\"Mean Temp\"))\n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ec68d3-c27b-4802-ab75-00f7a665be69",
   "metadata": {},
   "source": [
    "What you see is not the final results yet: you have to call the `collect()` method to obtain the final results. Actually, it is similar for `duckdb` -- nothing was actually computed until you call the `df()` method. You could have called `.pl()` instead of `.df()` to obtain a polars dataframe rather than a pandas dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dfe4873-5b11-4d1c-8a98-b443ffcb5380",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%time df.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186248f4-651d-4880-8d36-1e6ecbc417f8",
   "metadata": {},
   "source": [
    "What if we started from the `parquet` file? We download the station metadata file first if it's not there yet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abac948a-4683-47c2-bd04-b306e80685a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib\n",
    "# download station-metadata.csv if it does not exist\n",
    "url = \"https://raw.githubusercontent.com/PIC16B-ucla/24F/refs/heads/main/datasets/noaa-ghcn/station-metadata.csv\"\n",
    "if not os.path.exists(\"station-metadata.csv\"): \n",
    "    urllib.request.urlretrieve(url, \"station-metadata.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042d52b7-7055-4159-b1d2-914f94e6e885",
   "metadata": {},
   "source": [
    "Then run this lazy query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbde545-6a4a-4805-b10c-b76cf9779f5d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stations_pl = pl.scan_csv(\"station-metadata.csv\")\n",
    "df = (pl.scan_parquet(\"temps.parquet\")\n",
    "        .join(stations_pl, on=\"ID\", how=\"inner\")\n",
    "        .filter((pl.col(\"LATITUDE\") > 80) | (pl.col(\"LATITUDE\")< -80))\n",
    "        .group_by([\"NAME\", \"Month\"])\n",
    "        .agg(pl.col(\"Temp\").mean().alias(\"Mean Temp\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b04c383-e9e3-4ff2-81f8-82cb7b056cda",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%timeit df.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e772a22f-65ef-44b5-8457-7b8e04d5eed2",
   "metadata": {},
   "source": [
    "Oh, polars also supports some SQL!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe485c7-5a50-4c0c-9c43-8922e4b74bb3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%timeit\n",
    "ctx = pl.SQLContext(temperatures=pl.scan_parquet(\"temps.parquet\"), stations=pl.scan_csv(\"station-metadata.csv\"))\n",
    "cmd = \\\n",
    "\"\"\"\n",
    "SELECT S.NAME, T.Month, ROUND(AVG(T.Temp), 1) \"Mean Temperature\"\n",
    "FROM temperatures T\n",
    "LEFT JOIN stations S ON T.ID = S.ID\n",
    "WHERE S.LATITUDE>80 OR S.LATITUDE<-80\n",
    "GROUP BY S.NAME, T.Month\n",
    "\"\"\"\n",
    "rslt = ctx.execute(cmd)\n",
    "rslt.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21233cd-af5c-4c32-b9a4-e29ab3cb8636",
   "metadata": {},
   "source": [
    "All these numbers are remarkably faster than what we have seen in the past week! These two approaches will help you run fast as a data scientist."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:PIC16B-24F] *",
   "language": "python",
   "name": "conda-env-PIC16B-24F-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
